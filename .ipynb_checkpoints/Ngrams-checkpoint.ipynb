{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d97f7c5d-46f3-4cbd-80ad-f1e50cd65096",
   "metadata": {
    "tags": []
   },
   "source": [
    "# N-grams as Tokens for Phrases\n",
    "\n",
    "In this first project work we will use N-grams as tokens to create phrases in English and evaluate how much sense these phrases make."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb2b6d1-3df0-464c-9a5f-8c611257a971",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data preparation\n",
    "\n",
    "One of the most important parts of this project is preparing N-grams so that they can be used as tokens later on. There are two approaches:\n",
    "- using an existing dictionary of N-grams, such as Google Ngram;\n",
    "- creating a new one from a large corpus of text.\n",
    "We will be using the second approach since it would be easier to train our model on the topics we are interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45af26c3-7db4-4ff9-81c4-344e58bb4b8b",
   "metadata": {},
   "source": [
    "Another choice that has to be made is how many items do we want to consider for the N-grams? We will create a dictionary of **3 words** using NLTK library corpora. The main disadvantage of NLTK is that its corpora is quite limited in size and in application. But they are already cleaned and tokenized. This saves time on preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de291f3f-79f7-4a34-aca7-8f9b023a60f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.19.5 in c:\\users\\mo4al\\appdata\\roaming\\python\\python39\\site-packages (1.19.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy==1.19.5 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "957e0236-2f8b-45f9-bd62-b59729a5cd1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.util import ngrams\n",
    "import nltk\n",
    "from nltk.corpus import brown, gutenberg, reuters, inaugural, movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4de68d67-2d7a-4a2d-9a50-61ad0da6c4a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mo4al\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\mo4al\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\mo4al\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\mo4al\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package inaugural to\n",
      "[nltk_data]     C:\\Users\\mo4al\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\mo4al\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# punkt is a Sentence Tokenizer\n",
    "nltk.download('punkt')\n",
    "# Gutenberg Corpus includes public domain literary texts from authors like Shakespeare and Jane Austen\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "# more corpora that can be used to create the dictionary\n",
    "nltk.download('brown')\n",
    "nltk.download('reuters')\n",
    "nltk.download('inaugural')\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "# loading the corpus\n",
    "all_words = brown.words()\n",
    "    \n",
    "# generating bigrams (n=2) or trigrams (n=3)\n",
    "n = 3\n",
    "trigrams = list(ngrams(all_words, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca700401-b6ab-4d1b-9169-1f15df0c8209",
   "metadata": {},
   "source": [
    "If we want to work on a large-scale NLP project that requires substantial data, we might want to switch to a larger corpus like Common Crawl (hundreds of TB, but need to preprocess and clean), OpenWebText (about 40GB), or Wikipedia Dump (about 25GB).\n",
    "We would need to:\n",
    "- download the corpus;\n",
    "- preprocess removing tags, non article sections (if using Wikipedia);\n",
    "- tokenize sentences into words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9618d7f-7604-4e00-b049-3ea5a5762e66",
   "metadata": {},
   "source": [
    "A different code should be executed if we want to use an available N-gram dataset, for example Google Ngram Viewer.\n",
    "https://storage.googleapis.com/books/ngrams/books/datasetsv3.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3b6df7-e1fc-43b5-9e9b-44f3027b7e8e",
   "metadata": {},
   "source": [
    "### Building the model\n",
    "We want to create an N-gram language model using the data prepared in the first step. It is also possible to build a frequency-based model, where the likelihood of an n-gram appearing is proportional to its frequency in the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdf21208-3bb9-429f-853a-1f1bf8908ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trigram_model(trigrams):\n",
    "    # getting the frequency of each n-gram\n",
    "    ngram_freq = Counter(trigrams)\n",
    "\n",
    "    # creating a dictionary where each (n-1)-gram maps to possible next words\n",
    "    model = defaultdict(list)\n",
    "\n",
    "    # for trigrams, the prefix will be the first two words, and the next word will be the third\n",
    "    for (w1, w2, w3) in trigrams:\n",
    "        model[(w1, w2)].append(w3)\n",
    "\n",
    "    return ngram_freq, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d31e941-30ae-41c1-b7af-dcdfca168387",
   "metadata": {},
   "source": [
    "### Generating phrases\n",
    "We consider an initial word from which our phrase would start (in the dictionary it would be preceeded by special tokens to indicate their probable position at the beginning of the phrase), then we predict the next word by sampling from the probability distribution of possible next words given the previous n-1 words. We should continue this process until a phrase is of a certain length or a stop condition is met.\n",
    "The choice of the next word can be done using one of these approaches:\n",
    "- **greedy**: always pick the most probable word;\n",
    "- **random**: sample words based on probability in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3d5de6c-9091-400f-b8b9-9b8fea15042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_text(model, start_words, length=20):\n",
    "    text = list(start_words)\n",
    "    current_words = tuple(start_words)\n",
    "    \n",
    "    for _ in range(length):\n",
    "        if current_words in model:\n",
    "            \n",
    "            # Choose the next word based on its frequency distribution\n",
    "            next_word = random.choice(model[current_words])\n",
    "            text.append(next_word)\n",
    "            current_words = tuple(text[-(n-1):])\n",
    "        else:\n",
    "            break  # Stop if we no next word is available\n",
    "    \n",
    "    return ' '.join(text)\n",
    "\n",
    "def generate_greedy_text(model, start_words, length=20):\n",
    "    text = list(start_words)\n",
    "    current_words = tuple(start_words)\n",
    "    \n",
    "    for _ in range(length):\n",
    "        if current_words in model:\n",
    "            \n",
    "            # Select the most frequent next word\n",
    "            next_word = Counter(model[current_words]).most_common(1)[0][0]  # Get the most common next word\n",
    "            text.append(next_word)\n",
    "            current_words = tuple(text[-(n-1):])\n",
    "        else:\n",
    "            break  # Stop if no next word is available\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a251114c-9c96-49ce-96b5-185c61c8fbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the name of his materials themselves cannot sustain interest in playmates of\n",
      "the name of the United States , and the other hand ,\n"
     ]
    }
   ],
   "source": [
    "trigram_freq, model_ngram = create_trigram_model(trigrams)\n",
    "\n",
    "start_words = ('the', 'name')  # Start with any bigram (n-1) of your choice\n",
    "generated_random_text = generate_random_text(model_ngram, start_words, length=10)\n",
    "print(generated_random_text)\n",
    "\n",
    "generated_greedy_text = generate_greedy_text(model_ngram, start_words, length=10)\n",
    "print(generated_greedy_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec319af-a72d-419e-a2fb-7ac56e8d8ab7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluating phrases\n",
    "After generating a phrase we pass it to a LLM to evaluate its coherence and fluency, the result can be taken as is or can be used to further increase the model's performance. Some of the metrics to evaluate the phrases can be:\n",
    "- **perplexity** - calculate the perplexity, the lower the perplexity the better;\n",
    "- **semantic coherence** - use the LLM to assign a coherence score based on how logical or meaningful the phrase is;\n",
    "- **classifying phrases as coherent or not** - fine-tune a classification head on the LLM to check whether phrases make sense.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a53c65-bfb2-45e1-9a40-49f8e4171b98",
   "metadata": {},
   "source": [
    "We can use **GPT-2** to be the evaluator of the generated phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9324572-7876-49b6-9154-01ff81592304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (4.45.2)\n",
      "Requirement already satisfied: torch in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: torcheval in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (0.0.7)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: filelock in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mo4al\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: requests in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from torch) (2.7.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch sentencepiece torcheval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37a424a1-5f2c-49db-9c7a-19b6ac9b087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import sentencepiece as spm\n",
    "from torcheval.metrics.text import Perplexity\n",
    "\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3761492-1a04-420f-aa7a-bfce0313550d",
   "metadata": {},
   "source": [
    "Using **T5** and **HappyTextToText** to check and correct the phrase's grammar produced by the N-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "023a00eb-936a-4385-ac24-a0067aec5b3c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: happytransformer in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from happytransformer) (0.2.0)\n",
      "Requirement already satisfied: torch>=1.0 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from happytransformer) (2.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: wandb in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from happytransformer) (0.18.5)\n",
      "Requirement already satisfied: tqdm>=4.43 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from happytransformer) (4.66.5)\n",
      "\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.30.1 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from happytransformer) (4.45.2)\n",
      "Requirement already satisfied: datasets<3.0.0,>=2.13.1 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from happytransformer) (2.21.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from happytransformer) (3.19.1)\n",
      "Requirement already satisfied: accelerate<1.0.0,>=0.20.1 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from happytransformer) (0.34.2)\n",
      "Requirement already satisfied: tokenizers<1.0.0,>=0.13.3 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from happytransformer) (0.20.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from accelerate<1.0.0,>=0.20.1->happytransformer) (6.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from accelerate<1.0.0,>=0.20.1->happytransformer) (5.8.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from accelerate<1.0.0,>=0.20.1->happytransformer) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from accelerate<1.0.0,>=0.20.1->happytransformer) (0.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from accelerate<1.0.0,>=0.20.1->happytransformer) (21.3)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\mo4al\\appdata\\roaming\\python\\python39\\site-packages (from accelerate<1.0.0,>=0.20.1->happytransformer) (1.19.5)\n",
      "Requirement already satisfied: xxhash in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (3.5.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (3.8.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (17.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (0.70.16)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (2.32.3)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (0.3.8)\n",
      "Requirement already satisfied: fsspec[http]<=2024.6.1,>=2023.1.0 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (2024.6.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (1.4.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.13.1->happytransformer) (5.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.13.1->happytransformer) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.13.1->happytransformer) (1.6.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.13.1->happytransformer) (21.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.13.1->happytransformer) (2.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.13.1->happytransformer) (4.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.13.1->happytransformer) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from async-timeout<5.0,>=4.0.0a3->aiohttp->datasets<3.0.0,>=2.13.1->happytransformer) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from packaging>=20.0->accelerate<1.0.0,>=0.20.1->happytransformer) (3.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets<3.0.0,>=2.13.1->happytransformer) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets<3.0.0,>=2.13.1->happytransformer) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets<3.0.0,>=2.13.1->happytransformer) (2021.10.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from torch>=1.0->happytransformer) (2.11.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from torch>=1.0->happytransformer) (2.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from torch>=1.0->happytransformer) (1.10.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from tqdm>=4.43->happytransformer) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.30.1->happytransformer) (2022.3.15)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.0->happytransformer) (2.0.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from pandas->datasets<3.0.0,>=2.13.1->happytransformer) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from pandas->datasets<3.0.0,>=2.13.1->happytransformer) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets<3.0.0,>=2.13.1->happytransformer) (1.16.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from sympy->torch>=1.0->happytransformer) (1.2.1)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from wandb->happytransformer) (4.3.6)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from wandb->happytransformer) (8.0.4)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from wandb->happytransformer) (1.3.3)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from wandb->happytransformer) (0.4.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from wandb->happytransformer) (61.2.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from wandb->happytransformer) (3.1.43)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from wandb->happytransformer) (2.17.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb->happytransformer) (4.0.11)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\mo4al\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->happytransformer) (5.0.1)\n"
     ]
    }
   ],
   "source": [
    "pip install happytransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce31c8da-2c4c-4ced-8c5f-25237325d86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/23/2024 14:31:28 - INFO - happytransformer.happy_transformer -   Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from happytransformer import HappyTextToText, TTSettings\n",
    "happy_tt = HappyTextToText(\"T5\", \"vennify/t5-base-grammar-correction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32f8d754-9930-4cb5-bed7-337d12df9923",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/23/2024 14:31:34 - INFO - happytransformer.happy_transformer -   Moving model to cpu\n",
      "10/23/2024 14:31:34 - INFO - happytransformer.happy_transformer -   Initializing a pipeline\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the name of his materials themselves cannot sustain interest in playmates of\n",
      "The name of his materials itself cannot sustain interest in playmates.\n"
     ]
    }
   ],
   "source": [
    "def correct_grammar(input_phrase):\n",
    "    args = TTSettings(num_beams=5, min_length=1)\n",
    "\n",
    "    formatted_input = f\"grammar: {input_phrase}\"\n",
    "    result = happy_tt.generate_text(formatted_input, args=args)\n",
    "    \n",
    "    return result.text\n",
    "\n",
    "input_phrase = generated_random_text\n",
    "print(input_phrase)\n",
    "corrected_result = correct_grammar(input_phrase)\n",
    "print(corrected_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15ddcb16-38f0-4ed1-9063-9545cae21b7a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "model.to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2bf5514b-1b23-4afc-9bb9-7cbebb47a27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Phrase: the name of Luke Murrin . His sudden unannounced appearance at the\n",
      "Likelihood Score at the start: -5.517093181610107\n"
     ]
    }
   ],
   "source": [
    "def compute_phrase_likelihood(phrase):\n",
    "    input_ids = tokenizer(phrase, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "    \n",
    "    return -loss.item()\n",
    "\n",
    "phrase = generated_random_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07fccad7-858d-4798-83d8-dc302944de43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Phrase: The name of Luke Murrin . His sudden unannounced appearance at the event.\n",
      "Likelihood Score: -5.50302791595459\n"
     ]
    }
   ],
   "source": [
    "likelihood_score_initial = compute_phrase_likelihood(generated_)\n",
    "print(\"Initial Phrase:\", phrase)\n",
    "print(\"Likelihood Score at the start:\", likelihood_score_initial)\n",
    "\n",
    "likelihood_score_corrected = compute_phrase_likelihood(corrected_result)\n",
    "print(\"Corrected Phrase:\", corrected_result)\n",
    "print(\"Likelihood Score:\", likelihood_score_corrected)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
